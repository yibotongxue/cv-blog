---
head:
  - - link
    - rel: stylesheet
      href: https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css
---
# 密集不受约束的立体三维重建

本文介绍密集且不受约束的立体三维重建（DUSt3R），参考[论文](https://arxiv.org/abs/2312.14132)，参考的中文资料为[知乎文章](https://zhuanlan.zhihu.com/p/686078541)。所谓密集，即方法旨在为图片的全部或大多数像素建立三为对应；所谓不受约束，即表示方法不需要相机的先验知识，不需要对相机进行标定，不需要知道相机位姿等信息，而可以直接从输入的图像集重建出三维结构。

## ViT 介绍

在介绍立体三维重建之前，我们先介绍视觉转换器（Vision Transformer, ViT），参考[论文](https://arxiv.org/abs/2010.11929)和[知乎文章](https://zhuanlan.zhihu.com/p/640013974)。

### 网络结构介绍

转换器，亦指全自注意力网络等，即基于自注意力机制搭建的网络，根据特定的输入词元或称标记输出特征。其特出导致了自然语言处理的许多领域发生了巨大的进步，同时其也对于视觉领域产生了影响，ViT即其在图像分类上的体现。视觉转换器输入为图像，将其分块，然后通过嵌入层取得一个输入序列，再加入位置嵌入以标记位置信息，然后经过一个转换解码器，取得一个特征序列，再经过一个多层感知器转换头，得到分类结果。

### 分块嵌入

所谓分块，即将图片切分为若干个小块（比如 $16\times 16$ 的小块），所谓嵌入，即通过线性层转换为标记表示。实际实现中，可以使用卷积，令其步长无异于卷积核之大小，再对结果进行展平，即完成分块嵌入。

### 转换编码器

通过自注意力机制实现编码器，本质上与自然语言处理中的转换器中的编码器没有区分，因为我们上面已经得到了标记（词元），编码器事实上不需要区分标记的来处。基本的结构即先经过一个归一化层，再经过多头注意力层，其结果与归一化层的输入相加，再经过一个归一化层，并经过一个多层感知机，其结果再与归一化（第二个归一化）前的输入相加。基本的网络结构如下图所示，图片取自[知乎文章](https://zhuanlan.zhihu.com/p/640013974)

![transformer-encoder](https://picx.zhimg.com/v2-2a5dd1b501f927ea61e3c808ab319be9_1440w.jpg)

## DUSt3R 概述

既往的三维重建工作，运动重建结构经典的流程需要匹配关键点、进行捆绑调整等，立体三维视觉则往往需要假设相机参数作为输入，这些经典的方法往往都需要多个复杂的过程，而每一个过程的误差都可能带来巨大的影响。DUSt3R 则实现了从输入的非约束的图像，在不需要知道相机参数、相机位姿的前提下直接进行密集的立体三维重建得到点云，而不需要经过特征点提取、图相匹配、三角化等复杂的工作，并由得到的点云可以进行许多的下游的应用，比如密集三维重建、深度估计、相机标定等。

## 网络结构

这里讨论DUSt3R的网络结构，其输入应为两张RGB图像，输出则是两个点云及置信度，基本的结构可以参考下图，图片取自[知乎文章](https://zhuanlan.zhihu.com/p/686078541)

![dust3r](https://pic4.zhimg.com/v2-404927186a7459c388e8772264f40239_1440w.jpg)

概括其网络结构，即两张输入图像经过标记器后分别经过参数共享的ViT编码器，然后分别经过两个不同的解码器，两个解码器参数不同但通过交叉注意力机制交互，而后经过各自的转换头得到各自的点云和置信度分布。其中的点云是密集的，也即每一个像素得到一个三维坐标及其颜色等信息，置信度则表示每一个像素个别的点云信息的可信度。我们得到的两个点云，为了减少坐标变换，将坐标系选择为第一个相机的相机坐标系，而第二个图片得到的点云坐标也是在第一个相机的相机坐标系下。

损失函数的定义分为两个部分，一部分系预测点云与真实点云在欧氏空间的距离，并需要考虑置信度的因素；另一部分则为置信度的负对数，以期置信度尽可能地高。

需要注意的是，DUSt3R得到的点云没有尺度信息，为处理预测与真实值的尺度歧义，我们需要对其坐标除以一个归一化因子，为所有有效点到原点的平均距离。

## 下游应用

根据上面得到的网络，我们可以很容易取得两张RGB图像重建的三维点云，基于此我们可以解决很多三维视觉的问题。

### 图像匹配

图像匹配很容易实现，对于一张图像上的一个像素，比较另一张图像所有像素到其的欧氏空间距离，取最小者为匹配到的像素。

### 内参估计

假设光心居中，并有像素为正方形，从而只需要估计焦距，可以通过优化重投影误差实现，即

$$
f_1^* = argmin_{f_1}\sum\limits_{i=0}^{W}\sum\limits_{j=0}^{H}C_{i,j}^{1,1}||(i',j') - f_1\frac{(x_{i,j,0}^{1,1}, X_{i,j,1}^{1,1})}{X_{i,j,2}^{1,1}}||
$$

### 相对位姿估计

相对位姿估计有几种方法，一种是按照上面的步骤估计进行图相匹配、估计相机内参，然后通过估计极线矩阵然后恢复出相对位姿。另一种方法是直接通过优化的方法最小化一个点云投影与另一个点云的误差。比较稳健的方法是通过随机采样一致性和PnP方法实现。

### 绝对位姿估计

绝对位姿估计，也称视觉定位，输入查询图像和参考图像，可以先得到查询图像的内参估计，从图像得到运动关系（比如光流法），然后运行随机采样一致性和PnP方法，也可以如前所述得到相对位姿，然后通过参考图像与真实点云的比例，将这个位姿转换到世界坐标。

## 全局对齐

模型训练阶段是通过分别地输入两张图像得到的，在实际应用中，我们除了可以输入两张图像，也可以输入一张图像，即复制图像作为两个输入，也可以输入多张图像，即对每一个图像对进行三维重建，再行全局对齐。这里介绍全局对齐的内容。

对于输入的多张图像的图像集，将所有图像对经过网络得到点云和置信度，并剔除图像重合度太小的图像对，对于保留的所有图像对，设置一个姿态变换与缩放，令所有的图像对中的一者经过对应的姿态变换与缩放后与另一者的欧氏空间距离最小，并考虑置信度的因素。为避免得到平凡解，即缩放为 $0$ ，我们令所有的缩放乘积为 $1$ 。这与捆绑调整有些类似，不同的在于这里优化的是三维重投影误差。

基于这样的框架，我们除了实现了全局对齐，事实上还取得了相对位姿。我们注意到点云也可以表示为

$$
x_{i,j}^n := P_n^{-1}h(K_n^{-1}[iD_{i,j}^n;jD_{i,j}^n;D_{i,j}^n])
$$

从而我们可以估计出相机位姿、相机内参和深度图。

由于我们采用了优化三维重投影误差，并使用标准梯度下降，全局优化可以很快地收敛。
